import torch
import torch.nn as nn

from ..transformer import MultiHeadSelfAttention, TransformerLayer


class LVModel(nn.Module):
    """Custom Transformer classifier for PE malware detection (PyTorch)."""

    def __init__(self, vocab_size, embed_dim=128, num_heads=4,
                 ff_dim=256, num_layers=6, num_classes=2, max_seq_len=380, dropout_rate=0.1):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)
        self.pos_embed = nn.Embedding(max_seq_len, embed_dim)
        self.layers = nn.ModuleList([
            TransformerLayer(embed_dim, num_heads, ff_dim)
            for _ in range(num_layers)
        ])

        self.pooler = nn.Sequential(
            nn.Linear(embed_dim, embed_dim),
            nn.Tanh(),
            nn.Dropout(0.1),
        )
        self.classifier = nn.Sequential(
            nn.LayerNorm(embed_dim),
            nn.Linear(embed_dim, num_classes),
        )

        self.dropout = nn.Dropout(dropout_rate)
        self._init_weights()

    def forward(self, input_ids):
        seq_len = input_ids.size(1)
        positions = torch.arange(seq_len, device=input_ids.device).expand(input_ids.size(0), seq_len)

        x = self.embedding(input_ids) + self.pos_embed(positions)
        for layer in self.layers:
            x = layer(x)

        pooled = x.mean(dim=1)
        pooled = self.pooler(pooled)
        pooled = self.dropout(pooled)
        return self.classifier(pooled)

    def _init_weights(self):
        for module in self.modules():
            if isinstance(module, nn.Linear):
                nn.init.xavier_normal_(module.weight)
                if module.bias is not None:
                    nn.init.constant_(module.bias, 0)
            elif isinstance(module, nn.Embedding):
                nn.init.normal_(module.weight, mean=0, std=0.02)
