"""Flax LVModel with Multi-Similarity learning head."""

from typing import Optional

import jax.numpy as jnp
import flax.linen as nn
from flax.linen import initializers

from .transformer import FlaxTransformerLayer


class MultiSimilarityHead(nn.Module):
    num_classes: int
    embedding_dim: int

    def setup(self):
        kernel_init = initializers.variance_scaling(1.0, "fan_avg", "truncated_normal")
        self.classifier = nn.Dense(self.num_classes, kernel_init=kernel_init)

    def __call__(self, embeddings: jnp.ndarray, train: bool = True):
        return self.classifier(embeddings)


class FlaxLVModelWithMultiSimilarity(nn.Module):
    """Transformer model with Multi-Similarity loss head for malware detection."""

    vocab_size: int
    embed_dim: int = 128
    num_heads: int = 4
    ff_dim: int = 256
    num_layers: int = 6
    num_classes: int = 2
    max_seq_len: int = 380
    dropout_rate: float = 0.1
    embedding_dim: int = 256
    ms_alpha: float = 2.0
    ms_beta: float = 50.0
    ms_lambda: float = 1.0
    ms_margin: float = 0.1

    def setup(self):
        embed_init = initializers.normal(stddev=0.02)
        kernel_init = initializers.variance_scaling(1.0, "fan_avg", "truncated_normal")

        self.embedding = nn.Embed(num_embeddings=self.vocab_size, features=self.embed_dim, embedding_init=embed_init)
        self.pos_embed = nn.Embed(num_embeddings=self.max_seq_len, features=self.embed_dim, embedding_init=embed_init)

        self.encoder_layers = [
            FlaxTransformerLayer(embed_dim=self.embed_dim, num_heads=self.num_heads, ff_dim=self.ff_dim, dropout=self.dropout_rate)
            for _ in range(self.num_layers)
        ]

        self.pooler = nn.Dense(self.embed_dim, kernel_init=kernel_init)
        self.pooler_activation = nn.tanh
        self.pooler_dropout = nn.Dropout(rate=0.1)

        self.embedding_projection = nn.Dense(self.embedding_dim, kernel_init=kernel_init)
        self.embedding_norm = nn.LayerNorm()

        self.ms_head = MultiSimilarityHead(num_classes=self.num_classes, embedding_dim=self.embedding_dim)
        self.dropout = nn.Dropout(rate=self.dropout_rate)

    @nn.compact
    def __call__(self, input_ids, labels: Optional[jnp.ndarray] = None, train: bool = True,
                 return_embeddings: bool = False):
        seq_len = input_ids.shape[1]
        positions = jnp.arange(seq_len)[None, :].repeat(input_ids.shape[0], axis=0)

        x = self.embedding(input_ids) + self.pos_embed(positions)
        for layer in self.encoder_layers:
            x = layer(x, train=train)

        pooled = jnp.mean(x, axis=1)
        pooled = self.pooler(pooled)
        pooled = self.pooler_activation(pooled)
        pooled = self.pooler_dropout(pooled, deterministic=not train)

        embeddings = self.embedding_projection(pooled)
        embeddings = self.embedding_norm(embeddings)
        embeddings = embeddings / (jnp.linalg.norm(embeddings, axis=1, keepdims=True) + 1e-8)

        if return_embeddings:
            return embeddings

        logits = self.ms_head(embeddings, train=train)
        return logits
