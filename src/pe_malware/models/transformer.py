"""Shared Flax Transformer building blocks."""

import jax
import jax.numpy as jnp
import flax.linen as nn
from flax.linen import initializers


class FlaxMultiHeadSelfAttention(nn.Module):
    """Multi-head self-attention with combined QKV projection (Flax)."""

    embed_dim: int
    num_heads: int
    dropout: float = 0.1

    def setup(self):
        self.head_dim = self.embed_dim // self.num_heads
        assert self.embed_dim % self.num_heads == 0, "Embedding dimension must be divisible by number of heads."

        kernel_init = initializers.variance_scaling(1.0, "fan_avg", "truncated_normal")
        self.qkv = nn.Dense(3 * self.embed_dim, use_bias=True, kernel_init=kernel_init)
        self.fc_out = nn.Dense(self.embed_dim, kernel_init=kernel_init)
        self.dropout_layer = nn.Dropout(rate=self.dropout)
        self.scale = 1 / (self.head_dim ** 0.5)

    @nn.compact
    def __call__(self, x, train: bool = True):
        batch_size, seq_len, _ = x.shape

        qkv = self.qkv(x).reshape(batch_size, seq_len, 3, self.num_heads, self.head_dim)
        qkv = jnp.transpose(qkv, (2, 0, 3, 1, 4))
        q, k, v = qkv[0], qkv[1], qkv[2]

        attn = (q @ jnp.swapaxes(k, -2, -1)) * self.scale
        attn = jax.nn.softmax(attn, axis=-1)
        attn = self.dropout_layer(attn, deterministic=not train)

        out = (attn @ v).transpose(0, 2, 1, 3).reshape(batch_size, seq_len, -1)
        return self.fc_out(out)


class FlaxTransformerLayer(nn.Module):
    """Pre-normalization Transformer encoder layer (Flax)."""

    embed_dim: int
    num_heads: int
    ff_dim: int
    dropout: float = 0.1

    def setup(self):
        self.norm1 = nn.LayerNorm()
        self.attn = FlaxMultiHeadSelfAttention(
            embed_dim=self.embed_dim,
            num_heads=self.num_heads,
            dropout=self.dropout,
        )
        self.norm2 = nn.LayerNorm()
        self.dropout_layer = nn.Dropout(rate=self.dropout)

    @nn.compact
    def __call__(self, x, train: bool = True):
        x = x + self.attn(self.norm1(x), train=train)

        kernel_init = initializers.variance_scaling(1.0, "fan_avg", "truncated_normal")
        ffn_output = nn.Dense(self.ff_dim, kernel_init=kernel_init)(self.norm2(x))
        ffn_output = nn.gelu(ffn_output)
        ffn_output = self.dropout_layer(ffn_output, deterministic=not train)
        ffn_output = nn.Dense(self.embed_dim, kernel_init=kernel_init)(ffn_output)
        ffn_output = self.dropout_layer(ffn_output, deterministic=not train)

        x = x + ffn_output
        return x
