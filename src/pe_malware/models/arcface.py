"""Flax LVModel with ArcFace deep metric learning head."""

from typing import Optional

import jax
import jax.numpy as jnp
import flax.linen as nn
from flax.linen import initializers

from .transformer import FlaxTransformerLayer


class ArcFaceLayer(nn.Module):
    """ArcFace angular-margin classification layer.

    Adds an angular margin penalty in the cosine space to learn
    more discriminative embeddings.
    """

    num_classes: int
    embedding_dim: int
    margin: float = 0.5
    scale: float = 64.0

    def setup(self):
        kernel_init = initializers.variance_scaling(1.0, "fan_avg", "truncated_normal")
        self.weight = self.param('weight', kernel_init, (self.embedding_dim, self.num_classes))

    def __call__(self, embeddings: jnp.ndarray, labels: Optional[jnp.ndarray] = None, train: bool = True):
        embeddings_norm = embeddings / (jnp.linalg.norm(embeddings, axis=1, keepdims=True) + 1e-8)
        weight_norm = self.weight / (jnp.linalg.norm(self.weight, axis=0, keepdims=True) + 1e-8)
        cosine = jnp.matmul(embeddings_norm, weight_norm)

        if train and labels is not None:
            theta = jnp.arccos(jnp.clip(cosine, -1.0 + 1e-7, 1.0 - 1e-7))
            one_hot = jax.nn.one_hot(labels, self.num_classes)
            theta_m = theta + one_hot * self.margin
            cosine_m = jnp.cos(theta_m)
            logits = cosine_m * self.scale
        else:
            logits = cosine * self.scale

        return logits


class FlaxLVModelWithArcFace(nn.Module):
    """Transformer model with ArcFace for malware detection."""

    vocab_size: int
    embed_dim: int = 128
    num_heads: int = 4
    ff_dim: int = 256
    num_layers: int = 6
    num_classes: int = 2
    max_seq_len: int = 380
    dropout_rate: float = 0.1
    embedding_dim: int = 256
    arcface_margin: float = 0.5
    arcface_scale: float = 64.0

    def setup(self):
        embed_init = initializers.normal(stddev=0.02)
        kernel_init = initializers.variance_scaling(1.0, "fan_avg", "truncated_normal")

        self.embedding = nn.Embed(num_embeddings=self.vocab_size, features=self.embed_dim, embedding_init=embed_init)
        self.pos_embed = nn.Embed(num_embeddings=self.max_seq_len, features=self.embed_dim, embedding_init=embed_init)

        self.encoder_layers = [
            FlaxTransformerLayer(embed_dim=self.embed_dim, num_heads=self.num_heads, ff_dim=self.ff_dim, dropout=self.dropout_rate)
            for _ in range(self.num_layers)
        ]

        self.pooler = nn.Dense(self.embed_dim, kernel_init=kernel_init)
        self.pooler_activation = nn.tanh
        self.pooler_dropout = nn.Dropout(rate=0.1)

        self.embedding_projection = nn.Dense(self.embedding_dim, kernel_init=kernel_init)
        self.embedding_norm = nn.LayerNorm()

        self.arcface = ArcFaceLayer(
            num_classes=self.num_classes,
            embedding_dim=self.embedding_dim,
            margin=self.arcface_margin,
            scale=self.arcface_scale,
        )

        self.dropout = nn.Dropout(rate=self.dropout_rate)

    @nn.compact
    def __call__(self, input_ids, labels: Optional[jnp.ndarray] = None, train: bool = True,
                 return_embeddings: bool = False):
        seq_len = input_ids.shape[1]
        positions = jnp.arange(seq_len)[None, :].repeat(input_ids.shape[0], axis=0)

        x = self.embedding(input_ids) + self.pos_embed(positions)
        for layer in self.encoder_layers:
            x = layer(x, train=train)

        pooled = jnp.mean(x, axis=1)
        pooled = self.pooler(pooled)
        pooled = self.pooler_activation(pooled)
        pooled = self.pooler_dropout(pooled, deterministic=not train)

        embeddings = self.embedding_projection(pooled)
        embeddings = self.embedding_norm(embeddings)
        embeddings = embeddings / (jnp.linalg.norm(embeddings, axis=1, keepdims=True) + 1e-8)

        if return_embeddings:
            return embeddings

        logits = self.arcface(embeddings, labels=labels, train=train)
        return logits
