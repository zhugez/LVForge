"""Flax base model trainer with lax.scan optimization."""

import os
import logging
from functools import partial
from typing import Dict, Optional

import jax
import jax.numpy as jnp
import optax
import numpy as np
from flax.training import train_state
from flax import serialization
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score

from .losses import focal_loss
from ..evaluation.plotting import plot_training_metrics

logger = logging.getLogger(__name__)

BEST_MODEL_PATH = 'best_model_flax.msgpack'
FULL_MODEL_PATH = 'malware_detector_full_flax.msgpack'


class FlaxTrainer:
    """Trainer for FlaxLVModel with lax.scan optimization and Focal Loss support."""

    def __init__(self, model, params, device, num_epochs=1, patience=5, lr=2e-4,
                 batch_size=128, num_train_samples=None):
        self.device = device
        self.model = model
        self.num_epochs = num_epochs
        self.patience = patience
        self.batch_size = batch_size

        self.train_metrics = {m: [] for m in ['loss', 'accuracy', 'f1', 'precision', 'recall']}
        self.val_metrics = {m: [] for m in ['loss', 'accuracy', 'f1', 'precision', 'recall']}

        if num_train_samples is not None:
            steps_per_epoch = num_train_samples // batch_size
            decay_steps = num_epochs * steps_per_epoch
        else:
            decay_steps = num_epochs * 100

        schedule = optax.cosine_decay_schedule(init_value=lr, decay_steps=decay_steps, alpha=0.1)
        tx = optax.chain(optax.clip_by_global_norm(1.0), optax.adamw(learning_rate=schedule, weight_decay=0.01))

        self.state = train_state.TrainState.create(apply_fn=model.apply, params=params, tx=tx)
        self.best_metric = 0

    @staticmethod
    @partial(jax.jit, static_argnames=['model_apply', 'use_focal_loss'])
    def train_step(state, batch, dropout_key, model_apply, use_focal_loss: bool = False):
        def loss_fn(params):
            logits = model_apply({'params': params}, batch['input_ids'], train=True, rngs={'dropout': dropout_key})
            if use_focal_loss:
                loss = focal_loss(logits, batch['labels'])
            else:
                loss = optax.softmax_cross_entropy_with_integer_labels(logits, batch['labels']).mean()
            return loss, logits

        (loss, logits), grads = jax.value_and_grad(loss_fn, has_aux=True)(state.params)
        state = state.apply_gradients(grads=grads)
        preds = jnp.argmax(logits, axis=-1)
        accuracy = jnp.mean(preds == batch['labels'])
        return state, loss, accuracy

    @staticmethod
    @partial(jax.jit, static_argnames=['model_apply'])
    def eval_step(state, batch, model_apply):
        logits = model_apply({'params': state.params}, batch['input_ids'], train=False)
        loss = optax.softmax_cross_entropy_with_integer_labels(logits, batch['labels']).mean()
        return loss, logits

    def _run_epoch_scan(self, batched_data, dropout_key, training=True, use_focal_loss=False):
        if training:
            def scan_body(carry, batch):
                state, key = carry
                key, subkey = jax.random.split(key)
                state, loss, acc = self.train_step(state, batch, subkey, self.model.apply, use_focal_loss)
                return (state, key), (loss, acc)

            (self.state, _), (losses, _) = jax.lax.scan(scan_body, (self.state, dropout_key), batched_data)

            def pred_scan(carry, batch):
                logits = self.model.apply({'params': self.state.params}, batch['input_ids'], train=False)
                preds = jnp.argmax(logits, axis=-1)
                return carry, (preds, batch['labels'])

            _, (all_preds, all_targets) = jax.lax.scan(pred_scan, None, batched_data)
        else:
            def scan_body(carry, batch):
                loss, logits = self.eval_step(self.state, batch, self.model.apply)
                preds = jnp.argmax(logits, axis=-1)
                return carry, (loss, preds, batch['labels'])

            _, (losses, all_preds, all_targets) = jax.lax.scan(scan_body, None, batched_data)

        all_preds_np = np.array(all_preds.reshape(-1))
        all_targets_np = np.array(all_targets.reshape(-1))
        epoch_loss = jnp.mean(losses)

        return {
            'loss': float(epoch_loss),
            'accuracy': accuracy_score(all_targets_np, all_preds_np),
            'f1': f1_score(all_targets_np, all_preds_np, zero_division=0),
            'precision': precision_score(all_targets_np, all_preds_np, zero_division=0),
            'recall': recall_score(all_targets_np, all_preds_np, zero_division=0),
        }

    def train(self, tokenized_data_jax, labels_jax, vocab, tokenizer, padding_length, use_focal_loss=False):
        num_samples = len(labels_jax)
        key_shuffle = jax.random.PRNGKey(42)
        shuffle_idx = jax.random.permutation(key_shuffle, num_samples)
        tokenized_data_jax = tokenized_data_jax[shuffle_idx]
        labels_jax = labels_jax[shuffle_idx]

        unique, counts = np.unique(np.array(labels_jax), return_counts=True)
        logger.info(f"Class distribution: {dict(zip(unique, counts))}")

        split_idx = int(0.8 * num_samples)
        train_data = tokenized_data_jax[:split_idx]
        train_labels = labels_jax[:split_idx]
        val_data = tokenized_data_jax[split_idx:]
        val_labels = labels_jax[split_idx:]

        train_steps = len(train_labels) // self.batch_size
        train_size = train_steps * self.batch_size
        val_steps = len(val_labels) // self.batch_size
        val_size = val_steps * self.batch_size

        if len(train_labels) % self.batch_size != 0:
            logger.warning(f"Dropping {len(train_labels) - train_size} train samples due to batch truncation")

        key = jax.random.PRNGKey(42)
        dropout_key = jax.random.PRNGKey(43)
        patience_counter = 0

        if use_focal_loss:
            logger.info("Using Focal Loss (alpha=0.25, gamma=2.0)")

        for epoch in range(1, self.num_epochs + 1):
            key, subkey = jax.random.split(key)
            perms = jax.random.permutation(subkey, len(train_labels))[:train_size]

            batched_train = {
                'input_ids': train_data[perms].reshape(train_steps, self.batch_size, -1),
                'labels': train_labels[perms].reshape(train_steps, self.batch_size),
            }

            dropout_key, epoch_key = jax.random.split(dropout_key)
            train_stats = self._run_epoch_scan(batched_train, epoch_key, training=True, use_focal_loss=use_focal_loss)

            batched_val = {
                'input_ids': val_data[:val_size].reshape(val_steps, self.batch_size, -1),
                'labels': val_labels[:val_size].reshape(val_steps, self.batch_size),
            }
            val_stats = self._run_epoch_scan(batched_val, None, training=False)

            for metric in self.train_metrics:
                self.train_metrics[metric].append(train_stats[metric])
                self.val_metrics[metric].append(val_stats[metric])

            logger.info(f"\nEpoch {epoch}/{self.num_epochs}")
            logger.info(f"Train Loss: {train_stats['loss']:.8f} | Acc: {train_stats['accuracy']:.8f}")
            logger.info(f"Val Loss: {val_stats['loss']:.8f} | Acc: {val_stats['accuracy']:.8f}")
            logger.info(f"F1: {val_stats['f1']:.8f} | Precision: {val_stats['precision']:.8f} | Recall: {val_stats['recall']:.8f}")

            if val_stats['f1'] > self.best_metric:
                self.best_metric = val_stats['f1']
                patience_counter = 0
                self.save_checkpoint(BEST_MODEL_PATH)
                logger.info("Checkpoint saved: Best model updated.")
            else:
                patience_counter += 1
                if patience_counter >= self.patience:
                    logger.info("Early stopping triggered")
                    break

        if not os.path.exists(BEST_MODEL_PATH):
            self.save_checkpoint(BEST_MODEL_PATH)

        self.load_checkpoint(BEST_MODEL_PATH)
        plot_training_metrics(self.train_metrics, self.val_metrics, "training_metrics_flax.html")
        self._save_full_model(vocab, tokenizer, padding_length)
        logger.info("Training completed. Best model saved.")

    def save_checkpoint(self, filepath):
        bytes_output = serialization.to_bytes(self.state.params)
        with open(filepath, 'wb') as f:
            f.write(bytes_output)

    def load_checkpoint(self, filepath):
        with open(filepath, 'rb') as f:
            bytes_input = f.read()
        params = serialization.from_bytes(self.state.params, bytes_input)
        self.state = self.state.replace(params=params)

    def _save_full_model(self, vocab, tokenizer, padding_length, filepath=FULL_MODEL_PATH):
        import pickle
        checkpoint = {
            'params': serialization.to_bytes(self.state.params),
            'train_metrics': self.train_metrics,
            'val_metrics': self.val_metrics,
            'vocab': vocab,
            'max_seq_len': padding_length,
            'epoch': len(self.train_metrics['loss']),
        }
        with open(filepath, 'wb') as f:
            pickle.dump(checkpoint, f)
        logger.info(f"Full model state saved to {filepath}")
