"""Flax metric-learning trainer supporting ArcFace, Contrastive, Triplet, and Multi-Similarity."""

import os
import logging
from functools import partial
from typing import Dict, Optional

import jax
import jax.numpy as jnp
import optax
import numpy as np
from flax.training import train_state
from flax import serialization
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score

from .losses import (
    arcface_loss, focal_loss,
    combined_contrastive_loss,
    combined_triplet_loss,
    combined_ms_loss,
)
from ..evaluation.plotting import plot_training_metrics

logger = logging.getLogger(__name__)


class FlaxMetricTrainer:
    """Unified trainer for all Flax metric-learning model variants.

    Supported loss modes: 'arcface', 'contrastive', 'triplet', 'multi_similarity'.
    """

    def __init__(self, model, params, device, loss_mode='arcface',
                 num_epochs=1, patience=5, lr=2e-4, batch_size=128,
                 num_train_samples=None, **loss_kwargs):
        self.device = device
        self.model = model
        self.num_epochs = num_epochs
        self.patience = patience
        self.batch_size = batch_size
        self.loss_mode = loss_mode
        self.loss_kwargs = loss_kwargs

        self.train_metrics = {m: [] for m in ['loss', 'accuracy', 'f1', 'precision', 'recall']}
        self.val_metrics = {m: [] for m in ['loss', 'accuracy', 'f1', 'precision', 'recall']}

        if num_train_samples is not None:
            decay_steps = num_epochs * (num_train_samples // batch_size)
        else:
            decay_steps = num_epochs * 100

        schedule = optax.cosine_decay_schedule(init_value=lr, decay_steps=decay_steps, alpha=0.1)
        tx = optax.chain(optax.clip_by_global_norm(1.0), optax.adamw(learning_rate=schedule, weight_decay=0.01))

        self.state = train_state.TrainState.create(apply_fn=model.apply, params=params, tx=tx)
        self.best_metric = 0

        suffix = loss_mode
        self.best_path = f'best_model_flax_{suffix}.msgpack'
        self.full_path = f'malware_detector_full_flax_{suffix}.msgpack'

    # ---- ArcFace steps ----
    @staticmethod
    @partial(jax.jit, static_argnames=['model_apply', 'use_focal_loss'])
    def _train_step_arcface(state, batch, dropout_key, model_apply, use_focal_loss=False):
        def loss_fn(params):
            logits = model_apply({'params': params}, batch['input_ids'], labels=batch['labels'],
                                 train=True, rngs={'dropout': dropout_key})
            loss = focal_loss(logits, batch['labels']) if use_focal_loss else arcface_loss(logits, batch['labels'])
            return loss, logits
        (loss, logits), grads = jax.value_and_grad(loss_fn, has_aux=True)(state.params)
        state = state.apply_gradients(grads=grads)
        return state, loss, jnp.mean(jnp.argmax(logits, -1) == batch['labels'])

    # ---- Contrastive steps ----
    @staticmethod
    @partial(jax.jit, static_argnames=['model_apply', 'margin', 'alpha'])
    def _train_step_contrastive(state, batch, dropout_key, model_apply, margin=1.0, alpha=0.5):
        def loss_fn(params):
            logits = model_apply({'params': params}, batch['input_ids'], labels=batch['labels'],
                                 train=True, rngs={'dropout': dropout_key})
            emb = model_apply({'params': params}, batch['input_ids'], train=True,
                              return_embeddings=True, rngs={'dropout': dropout_key})
            loss = combined_contrastive_loss(logits, emb, batch['labels'], margin, alpha)
            return loss, logits
        (loss, logits), grads = jax.value_and_grad(loss_fn, has_aux=True)(state.params)
        state = state.apply_gradients(grads=grads)
        return state, loss, jnp.mean(jnp.argmax(logits, -1) == batch['labels'])

    # ---- Triplet steps ----
    @staticmethod
    @partial(jax.jit, static_argnames=['model_apply', 'margin', 'alpha', 'use_hard_mining'])
    def _train_step_triplet(state, batch, dropout_key, model_apply, margin=0.3, alpha=0.5, use_hard_mining=True):
        def loss_fn(params):
            logits = model_apply({'params': params}, batch['input_ids'], labels=batch['labels'],
                                 train=True, rngs={'dropout': dropout_key})
            emb = model_apply({'params': params}, batch['input_ids'], train=True,
                              return_embeddings=True, rngs={'dropout': dropout_key})
            loss = combined_triplet_loss(logits, emb, batch['labels'], margin, alpha, use_hard_mining)
            return loss, logits
        (loss, logits), grads = jax.value_and_grad(loss_fn, has_aux=True)(state.params)
        state = state.apply_gradients(grads=grads)
        return state, loss, jnp.mean(jnp.argmax(logits, -1) == batch['labels'])

    # ---- Multi-Similarity steps ----
    @staticmethod
    @partial(jax.jit, static_argnames=['model_apply', 'ms_alpha', 'ms_beta', 'ms_lambda', 'ms_margin', 'ce_weight'])
    def _train_step_ms(state, batch, dropout_key, model_apply,
                       ms_alpha=2.0, ms_beta=50.0, ms_lambda=1.0, ms_margin=0.1, ce_weight=0.5):
        def loss_fn(params):
            logits = model_apply({'params': params}, batch['input_ids'], labels=batch['labels'],
                                 train=True, rngs={'dropout': dropout_key})
            emb = model_apply({'params': params}, batch['input_ids'], train=True,
                              return_embeddings=True, rngs={'dropout': dropout_key})
            loss = combined_ms_loss(logits, emb, batch['labels'], ms_alpha, ms_beta, ms_lambda, ms_margin, ce_weight)
            return loss, logits
        (loss, logits), grads = jax.value_and_grad(loss_fn, has_aux=True)(state.params)
        state = state.apply_gradients(grads=grads)
        return state, loss, jnp.mean(jnp.argmax(logits, -1) == batch['labels'])

    # ---- Shared eval step ----
    @staticmethod
    @partial(jax.jit, static_argnames=['model_apply'])
    def eval_step(state, batch, model_apply):
        logits = model_apply({'params': state.params}, batch['input_ids'], labels=None, train=False)
        loss = optax.softmax_cross_entropy_with_integer_labels(logits, batch['labels']).mean()
        return loss, logits

    def _get_scan_body(self, dropout_key):
        """Return the appropriate scan body for the configured loss mode."""
        kw = self.loss_kwargs

        if self.loss_mode == 'arcface':
            def body(carry, batch):
                state, key = carry
                key, subkey = jax.random.split(key)
                state, loss, acc = self._train_step_arcface(
                    state, batch, subkey, self.model.apply, kw.get('use_focal_loss', False))
                return (state, key), (loss, acc)
        elif self.loss_mode == 'contrastive':
            def body(carry, batch):
                state, key = carry
                key, subkey = jax.random.split(key)
                state, loss, acc = self._train_step_contrastive(
                    state, batch, subkey, self.model.apply,
                    kw.get('margin', 1.0), kw.get('alpha', 0.5))
                return (state, key), (loss, acc)
        elif self.loss_mode == 'triplet':
            def body(carry, batch):
                state, key = carry
                key, subkey = jax.random.split(key)
                state, loss, acc = self._train_step_triplet(
                    state, batch, subkey, self.model.apply,
                    kw.get('margin', 0.3), kw.get('alpha', 0.5), kw.get('use_hard_mining', True))
                return (state, key), (loss, acc)
        elif self.loss_mode == 'multi_similarity':
            def body(carry, batch):
                state, key = carry
                key, subkey = jax.random.split(key)
                state, loss, acc = self._train_step_ms(
                    state, batch, subkey, self.model.apply,
                    kw.get('ms_alpha', 2.0), kw.get('ms_beta', 50.0),
                    kw.get('ms_lambda', 1.0), kw.get('ms_margin', 0.1), kw.get('ce_weight', 0.5))
                return (state, key), (loss, acc)
        else:
            raise ValueError(f"Unknown loss mode: {self.loss_mode}")

        return body

    def _run_epoch_scan(self, batched_data, dropout_key, training=True):
        if training:
            scan_body = self._get_scan_body(dropout_key)
            (self.state, _), (losses, _) = jax.lax.scan(scan_body, (self.state, dropout_key), batched_data)

            def pred_scan(carry, batch):
                logits = self.model.apply({'params': self.state.params}, batch['input_ids'], labels=None, train=False)
                preds = jnp.argmax(logits, axis=-1)
                return carry, (preds, batch['labels'])
            _, (all_preds, all_targets) = jax.lax.scan(pred_scan, None, batched_data)
        else:
            def scan_body(carry, batch):
                loss, logits = self.eval_step(self.state, batch, self.model.apply)
                preds = jnp.argmax(logits, axis=-1)
                return carry, (loss, preds, batch['labels'])
            _, (losses, all_preds, all_targets) = jax.lax.scan(scan_body, None, batched_data)

        all_preds_np = np.array(all_preds.reshape(-1))
        all_targets_np = np.array(all_targets.reshape(-1))

        return {
            'loss': float(jnp.mean(losses)),
            'accuracy': accuracy_score(all_targets_np, all_preds_np),
            'f1': f1_score(all_targets_np, all_preds_np, zero_division=0),
            'precision': precision_score(all_targets_np, all_preds_np, zero_division=0),
            'recall': recall_score(all_targets_np, all_preds_np, zero_division=0),
        }

    def train(self, tokenized_data_jax, labels_jax, vocab, tokenizer, padding_length):
        num_samples = len(labels_jax)
        key_shuffle = jax.random.PRNGKey(42)
        shuffle_idx = jax.random.permutation(key_shuffle, num_samples)
        tokenized_data_jax = tokenized_data_jax[shuffle_idx]
        labels_jax = labels_jax[shuffle_idx]

        unique, counts = np.unique(np.array(labels_jax), return_counts=True)
        logger.info(f"Class distribution: {dict(zip(unique, counts))}")

        split_idx = int(0.8 * num_samples)
        train_data, train_labels = tokenized_data_jax[:split_idx], labels_jax[:split_idx]
        val_data, val_labels = tokenized_data_jax[split_idx:], labels_jax[split_idx:]

        train_steps = len(train_labels) // self.batch_size
        train_size = train_steps * self.batch_size
        val_steps = len(val_labels) // self.batch_size
        val_size = val_steps * self.batch_size

        key = jax.random.PRNGKey(42)
        dropout_key = jax.random.PRNGKey(43)
        patience_counter = 0

        logger.info(f"Using {self.loss_mode} loss (kwargs: {self.loss_kwargs})")

        for epoch in range(1, self.num_epochs + 1):
            key, subkey = jax.random.split(key)
            perms = jax.random.permutation(subkey, len(train_labels))[:train_size]
            batched_train = {
                'input_ids': train_data[perms].reshape(train_steps, self.batch_size, -1),
                'labels': train_labels[perms].reshape(train_steps, self.batch_size),
            }
            dropout_key, epoch_key = jax.random.split(dropout_key)
            train_stats = self._run_epoch_scan(batched_train, epoch_key, training=True)

            batched_val = {
                'input_ids': val_data[:val_size].reshape(val_steps, self.batch_size, -1),
                'labels': val_labels[:val_size].reshape(val_steps, self.batch_size),
            }
            val_stats = self._run_epoch_scan(batched_val, None, training=False)

            for m in self.train_metrics:
                self.train_metrics[m].append(train_stats[m])
                self.val_metrics[m].append(val_stats[m])

            logger.info(f"\nEpoch {epoch}/{self.num_epochs}")
            logger.info(f"Train Loss: {train_stats['loss']:.8f} | Acc: {train_stats['accuracy']:.8f}")
            logger.info(f"Val Loss: {val_stats['loss']:.8f} | Acc: {val_stats['accuracy']:.8f}")
            logger.info(f"F1: {val_stats['f1']:.8f} | Precision: {val_stats['precision']:.8f} | Recall: {val_stats['recall']:.8f}")

            if val_stats['f1'] > self.best_metric:
                self.best_metric = val_stats['f1']
                patience_counter = 0
                self.save_checkpoint(self.best_path)
                logger.info("Checkpoint saved: Best model updated.")
            else:
                patience_counter += 1
                if patience_counter >= self.patience:
                    logger.info("Early stopping triggered")
                    break

        if not os.path.exists(self.best_path):
            self.save_checkpoint(self.best_path)

        self.load_checkpoint(self.best_path)
        plot_training_metrics(self.train_metrics, self.val_metrics, f"training_metrics_flax_{self.loss_mode}.html",
                              title=f"Training Metrics ({self.loss_mode.replace('_', ' ').title()})")
        self._save_full_model(vocab, tokenizer, padding_length)
        logger.info("Training completed. Best model saved.")

    def save_checkpoint(self, filepath):
        data = serialization.to_bytes(self.state.params)
        with open(filepath, 'wb') as f:
            f.write(data)

    def load_checkpoint(self, filepath):
        with open(filepath, 'rb') as f:
            data = f.read()
        params = serialization.from_bytes(self.state.params, data)
        self.state = self.state.replace(params=params)

    def _save_full_model(self, vocab, tokenizer, padding_length):
        import pickle
        checkpoint = {
            'params': serialization.to_bytes(self.state.params),
            'train_metrics': self.train_metrics,
            'val_metrics': self.val_metrics,
            'vocab': vocab,
            'max_seq_len': padding_length,
            'epoch': len(self.train_metrics['loss']),
        }
        with open(self.full_path, 'wb') as f:
            pickle.dump(checkpoint, f)
        logger.info(f"Full model state saved to {self.full_path}")
