"""PyTorch model trainer with early stopping and metric tracking."""

import os
import logging

import torch
import torch.nn as nn
import torch.optim as optim
import torch.cuda.amp as amp
from torch.utils.data import DataLoader, random_split
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score

from ..evaluation.plotting import plot_training_metrics

logger = logging.getLogger(__name__)

BEST_MODEL_PATH = 'best_model.pth'
FULL_MODEL_PATH = 'malware_detector_full.pth'


class PyTorchTrainer:
    """Trainer for PyTorch LVModel with mixed precision and early stopping."""

    def __init__(self, model, train_loader, val_loader, device,
                 num_epochs=1, patience=5, lr=2e-4):
        self.device = device
        self.model = model.to(device)
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.num_epochs = num_epochs
        self.patience = patience

        self.train_metrics = {m: [] for m in ['loss', 'accuracy', 'f1', 'precision', 'recall']}
        self.val_metrics = {m: [] for m in ['loss', 'accuracy', 'f1', 'precision', 'recall']}

        self.criterion = nn.CrossEntropyLoss()
        self.optimizer = optim.AdamW(self.model.parameters(), lr=lr, weight_decay=0.01)
        self.scheduler = optim.lr_scheduler.CosineAnnealingLR(self.optimizer, T_max=num_epochs)
        self.scaler = amp.GradScaler()
        self.best_metric = 0

    def _run_epoch(self, loader, training=True):
        self.model.train(mode=training)
        total_loss = 0
        all_preds, all_targets = [], []

        with torch.set_grad_enabled(training):
            for inputs, targets in loader:
                inputs, targets = inputs.to(self.device), targets.to(self.device)
                with amp.autocast():
                    outputs = self.model(inputs)
                    loss = self.criterion(outputs, targets)
                if training:
                    self.optimizer.zero_grad()
                    self.scaler.scale(loss).backward()
                    nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                    self.scaler.step(self.optimizer)
                    self.scaler.update()
                total_loss += loss.item() * targets.size(0)
                preds = outputs.argmax(dim=1)
                all_preds.extend(preds.cpu().numpy())
                all_targets.extend(targets.cpu().numpy())

        epoch_loss = total_loss / len(loader.dataset)
        return {
            'loss': epoch_loss,
            'accuracy': accuracy_score(all_targets, all_preds),
            'f1': f1_score(all_targets, all_preds),
            'precision': precision_score(all_targets, all_preds),
            'recall': recall_score(all_targets, all_preds),
        }

    def train(self, vocab, tokenizer, padding_length):
        patience_counter = 0
        torch.backends.cudnn.benchmark = True
        self.scaler = amp.GradScaler(enabled=self.device.type == 'cuda')

        for epoch in range(1, self.num_epochs + 1):
            train_stats = self._run_epoch(self.train_loader, training=True)
            val_stats = self._run_epoch(self.val_loader, training=False)
            self.scheduler.step()

            for metric in self.train_metrics:
                self.train_metrics[metric].append(train_stats[metric])
                self.val_metrics[metric].append(val_stats[metric])

            logger.info(f"\nEpoch {epoch}/{self.num_epochs}")
            logger.info(f"Train Loss: {train_stats['loss']:.4f} | Acc: {train_stats['accuracy']:.2%}")
            logger.info(f"Val Loss: {val_stats['loss']:.4f} | Acc: {val_stats['accuracy']:.2%}")
            logger.info(f"F1: {val_stats['f1']:.4f} | Precision: {val_stats['precision']:.4f} | Recall: {val_stats['recall']:.4f}")

            if val_stats['f1'] > self.best_metric:
                self.best_metric = val_stats['f1']
                patience_counter = 0
                torch.save(self.model.state_dict(), BEST_MODEL_PATH)
                logger.info("Checkpoint saved: Best model updated.")
            else:
                patience_counter += 1
                if patience_counter >= self.patience:
                    logger.info("Early stopping triggered")
                    break

        if not os.path.exists(BEST_MODEL_PATH):
            logger.info("No improvement observed. Saving final model state as checkpoint.")
            torch.save(self.model.state_dict(), BEST_MODEL_PATH)

        self.model.load_state_dict(torch.load(BEST_MODEL_PATH))
        plot_training_metrics(self.train_metrics, self.val_metrics, "training_metrics.html")
        self._save_full_model(vocab, tokenizer, padding_length)
        logger.info("Training completed. Best model saved.")

    def _save_full_model(self, vocab, tokenizer, padding_length, filepath=FULL_MODEL_PATH):
        checkpoint = {
            'model_state': self.model.state_dict(),
            'optimizer_state': self.optimizer.state_dict(),
            'scheduler_state': self.scheduler.state_dict(),
            'train_metrics': self.train_metrics,
            'val_metrics': self.val_metrics,
            'vocab': vocab,
            'tokenizer_config': tokenizer.config,
            'max_seq_len': padding_length,
            'epoch': len(self.train_metrics['loss']),
        }
        torch.save(checkpoint, filepath)
        logger.info(f"Full model state saved to {filepath}")
