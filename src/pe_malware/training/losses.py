"""Loss functions for PE malware detection models."""

import jax
import jax.numpy as jnp
import optax


def focal_loss(logits: jnp.ndarray, labels: jnp.ndarray, alpha: float = 0.25, gamma: float = 2.0) -> jnp.ndarray:
    """Focal Loss for handling class imbalance."""
    ce_loss = optax.softmax_cross_entropy_with_integer_labels(logits, labels)
    p = jax.nn.softmax(logits, axis=-1)
    p_t = jnp.take_along_axis(p, labels[:, None], axis=1).squeeze(1)
    focal_weight = alpha * (1 - p_t) ** gamma
    return jnp.mean(focal_weight * ce_loss)


def arcface_loss(logits: jnp.ndarray, labels: jnp.ndarray) -> jnp.ndarray:
    """Cross-entropy on ArcFace logits (margin already applied in model)."""
    return optax.softmax_cross_entropy_with_integer_labels(logits, labels).mean()


def contrastive_loss(embeddings: jnp.ndarray, labels: jnp.ndarray, margin: float = 1.0) -> jnp.ndarray:
    """Contrastive loss on pairwise distances."""
    batch_size = embeddings.shape[0]
    embeddings_norm = embeddings / (jnp.linalg.norm(embeddings, axis=1, keepdims=True) + 1e-8)
    distances = jnp.sqrt(jnp.sum((embeddings_norm[:, None, :] - embeddings_norm[None, :, :]) ** 2, axis=-1) + 1e-8)
    labels_equal = labels[:, None] == labels[None, :]
    mask = ~jnp.eye(batch_size, dtype=bool)

    positive_loss = labels_equal * distances ** 2
    negative_loss = (~labels_equal) * jnp.maximum(margin - distances, 0) ** 2
    loss = (positive_loss + negative_loss) * mask
    num_valid = jnp.sum(mask)
    return jnp.sum(loss) / (num_valid + 1e-8)


def combined_contrastive_loss(logits, embeddings, labels, margin=1.0, alpha=0.5):
    """Combined cross-entropy + contrastive loss."""
    ce = optax.softmax_cross_entropy_with_integer_labels(logits, labels).mean()
    contr = contrastive_loss(embeddings, labels, margin)
    return alpha * ce + (1 - alpha) * contr


def batch_hard_triplet_loss(embeddings: jnp.ndarray, labels: jnp.ndarray, margin: float = 0.3) -> jnp.ndarray:
    """Batch-hard triplet loss with online hard mining."""
    batch_size = embeddings.shape[0]
    embeddings_norm = embeddings / (jnp.linalg.norm(embeddings, axis=1, keepdims=True) + 1e-8)
    pairwise_dist = jnp.sum((embeddings_norm[:, None, :] - embeddings_norm[None, :, :]) ** 2, axis=-1)

    labels_equal = labels[:, None] == labels[None, :]
    labels_not_equal = ~labels_equal
    mask_anchor_positive = labels_equal & ~jnp.eye(batch_size, dtype=bool)

    hardest_positive_dist = jnp.where(mask_anchor_positive, pairwise_dist, jnp.zeros_like(pairwise_dist)).max(axis=1)
    max_dist = jnp.max(pairwise_dist)
    hardest_negative_dist = jnp.where(labels_not_equal, pairwise_dist, jnp.ones_like(pairwise_dist) * (max_dist + 1)).min(axis=1)

    triplet_loss = jnp.maximum(hardest_positive_dist - hardest_negative_dist + margin, 0.0)
    valid_triplets = jnp.any(mask_anchor_positive, axis=1)
    num_valid = jnp.sum(valid_triplets) + 1e-8
    return jnp.sum(triplet_loss * valid_triplets) / num_valid


def batch_all_triplet_loss(embeddings: jnp.ndarray, labels: jnp.ndarray, margin: float = 0.3) -> jnp.ndarray:
    """Batch-all triplet loss."""
    batch_size = embeddings.shape[0]
    embeddings_norm = embeddings / (jnp.linalg.norm(embeddings, axis=1, keepdims=True) + 1e-8)
    pairwise_dist = jnp.sum((embeddings_norm[:, None, :] - embeddings_norm[None, :, :]) ** 2, axis=-1)

    anchor_positive_dist = pairwise_dist[:, :, None]
    anchor_negative_dist = pairwise_dist[:, None, :]
    triplet_loss = anchor_positive_dist - anchor_negative_dist + margin

    labels_equal = labels[:, None] == labels[None, :]
    i_equal_j = labels_equal[:, :, None]
    i_equal_k = labels_equal[:, None, :]
    valid_mask = i_equal_j & ~i_equal_k

    i_not_j = ~jnp.eye(batch_size, dtype=bool)[:, :, None]
    i_not_k = ~jnp.eye(batch_size, dtype=bool)[:, None, :]
    j_not_k = ~jnp.eye(batch_size, dtype=bool)[None, :, :]
    distinct_mask = i_not_j & i_not_k & j_not_k
    mask = valid_mask & distinct_mask

    triplet_loss = jnp.maximum(triplet_loss * mask, 0.0)
    num_valid = jnp.sum(mask) + 1e-8
    return jnp.sum(triplet_loss) / num_valid


def combined_triplet_loss(logits, embeddings, labels, margin=0.3, alpha=0.5, use_hard_mining=True):
    """Combined cross-entropy + triplet loss."""
    ce = optax.softmax_cross_entropy_with_integer_labels(logits, labels).mean()
    tri = batch_hard_triplet_loss(embeddings, labels, margin) if use_hard_mining else batch_all_triplet_loss(embeddings, labels, margin)
    return alpha * ce + (1 - alpha) * tri


def multi_similarity_loss_vectorized(embeddings: jnp.ndarray, labels: jnp.ndarray,
                                      alpha: float = 2.0, beta: float = 50.0,
                                      ms_lambda: float = 1.0, margin: float = 0.1) -> jnp.ndarray:
    """Vectorized Multi-Similarity loss."""
    batch_size = embeddings.shape[0]
    embeddings_norm = embeddings / (jnp.linalg.norm(embeddings, axis=1, keepdims=True) + 1e-8)
    similarity = jnp.matmul(embeddings_norm, embeddings_norm.T)

    labels_equal = labels[:, None] == labels[None, :]
    mask_pos = labels_equal & ~jnp.eye(batch_size, dtype=bool)
    mask_neg = ~labels_equal

    neg_sim_for_hard = jnp.where(mask_neg, similarity, -1e9)
    hard_neg_max = jnp.max(neg_sim_for_hard, axis=1, keepdims=True)
    pos_sim_for_hard = jnp.where(mask_pos, similarity, 1e9)
    hard_pos_min = jnp.min(pos_sim_for_hard, axis=1, keepdims=True)

    pos_mask_hard = mask_pos & (similarity < hard_neg_max + margin)
    neg_mask_hard = mask_neg & (similarity > hard_pos_min - margin)

    pos_exp = jnp.exp(-alpha * (similarity - ms_lambda))
    neg_exp = jnp.exp(beta * (similarity - ms_lambda))

    pos_sum = jnp.sum(jnp.where(pos_mask_hard, pos_exp, 0.0), axis=1)
    neg_sum = jnp.sum(jnp.where(neg_mask_hard, neg_exp, 0.0), axis=1)

    has_pos = jnp.any(mask_pos, axis=1)
    has_neg = jnp.any(mask_neg, axis=1)

    pos_loss = jnp.where(has_pos & (pos_sum > 0), jnp.log1p(pos_sum) / alpha, 0.0)
    neg_loss = jnp.where(has_neg & (neg_sum > 0), jnp.log1p(neg_sum) / beta, 0.0)

    valid_samples = has_pos | has_neg
    num_valid = jnp.sum(valid_samples) + 1e-8
    return jnp.sum((pos_loss + neg_loss) * valid_samples) / num_valid


def combined_ms_loss(logits, embeddings, labels, alpha=2.0, beta=50.0, ms_lambda=1.0,
                     margin=0.1, ce_weight=0.5):
    """Combined cross-entropy + multi-similarity loss."""
    ce = optax.softmax_cross_entropy_with_integer_labels(logits, labels).mean()
    ms = multi_similarity_loss_vectorized(embeddings, labels, alpha, beta, ms_lambda, margin)
    return ce_weight * ce + (1 - ce_weight) * ms
