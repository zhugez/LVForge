"""Evaluation metrics: threshold tuning, multi-seed analysis, and statistical summary."""

import json
import logging
from functools import partial

import jax
import jax.numpy as jnp
import numpy as np
import optax
from sklearn.metrics import (
    accuracy_score, f1_score, precision_score, recall_score,
    precision_recall_curve, classification_report, confusion_matrix,
    roc_curve, auc, roc_auc_score,
)

from .plotting import plot_roc_pr_curves

logger = logging.getLogger(__name__)


def _get_val_predictions(trainer, data_jax, labels_jax, batch_size, seed=42):
    """Shuffle, split 80/20, batch, and get val predictions."""
    num_samples = len(labels_jax)
    key = jax.random.PRNGKey(seed)
    shuffle_idx = jax.random.permutation(key, num_samples)
    data_jax = data_jax[shuffle_idx]
    labels_jax = labels_jax[shuffle_idx]

    split_idx = int(0.8 * num_samples)
    val_data = data_jax[split_idx:]
    val_labels = labels_jax[split_idx:]

    val_steps = len(val_labels) // batch_size
    val_size = val_steps * batch_size

    val_data_batched = val_data[:val_size].reshape(val_steps, batch_size, -1)
    val_labels_batched = val_labels[:val_size].reshape(val_steps, batch_size)
    batched = {'input_ids': val_data_batched, 'labels': val_labels_batched}

    @partial(jax.jit, static_argnames=['model_apply'])
    def _eval_step(state, batch, model_apply):
        logits = model_apply({'params': state.params}, batch['input_ids'], labels=None, train=False)
        return logits, batch['labels']

    def scan_body(carry, batch):
        logits, labels = _eval_step(trainer.state, batch, trainer.model.apply)
        return carry, (logits, labels)

    _, (logits_list, labels_list) = jax.lax.scan(scan_body, None, batched)

    logits_flat = logits_list.reshape(-1, logits_list.shape[-1])
    labels_flat = labels_list.reshape(-1)
    probs = jax.nn.softmax(logits_flat, axis=-1)[:, 1]

    return np.array(labels_flat), np.array(probs)


def evaluate_model(trainer, data_jax, labels_jax, batch_size=128, suffix=""):
    """Full evaluation: threshold tuning, classification report, TPR@FPR, curves."""
    y_true, y_score = _get_val_predictions(trainer, data_jax, labels_jax, batch_size)

    precisions, recalls, thresholds = precision_recall_curve(y_true, y_score)
    f1_scores = 2 * (precisions * recalls) / (precisions + recalls + 1e-10)
    best_idx = np.argmax(f1_scores)
    best_threshold = thresholds[best_idx] if best_idx < len(thresholds) else 0.5
    logger.info(f"Optimal Threshold: {best_threshold:.8f}")

    y_pred = (y_score > best_threshold).astype(int)
    print("\n" + classification_report(y_true, y_pred, target_names=['Benign', 'Malware'], digits=8))

    cm = confusion_matrix(y_true, y_pred)
    logger.info(f"Confusion Matrix:\n{cm}")

    fpr, tpr, _ = roc_curve(y_true, y_score)
    logger.info("--- Advanced Metrics ---")
    for target_fpr in [1e-2, 1e-3, 1e-4]:
        idx = np.where(fpr <= target_fpr)[0]
        if len(idx) > 0:
            logger.info(f"TPR @ FPR={target_fpr:.0e}: {tpr[idx[-1]]:.8f}")
        else:
            logger.info(f"TPR @ FPR={target_fpr:.0e}: N/A")

    plot_roc_pr_curves(y_true, y_score, suffix=suffix)


def evaluate_multi_seed(trainer, data_jax, labels_jax, batch_size=128,
                         seeds=None, suffix=""):
    """Multi-seed evaluation with statistical summary."""
    if seeds is None:
        seeds = [42, 43, 44, 45, 46]

    logger.info(f"Running {len(seeds)}-seed evaluation for statistical rigor...")

    result_keys = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc', 'pr_auc',
                   'tpr_at_fpr_1e2', 'tpr_at_fpr_1e3', 'tpr_at_fpr_1e4', 'optimal_threshold']
    results = {k: [] for k in result_keys}

    for seed in seeds:
        logger.info(f"\nEvaluating with seed {seed}...")
        y_true, y_score = _get_val_predictions(trainer, data_jax, labels_jax, batch_size, seed=seed)

        precisions, recalls, thresholds = precision_recall_curve(y_true, y_score)
        f1_scores = 2 * (precisions * recalls) / (precisions + recalls + 1e-10)
        best_idx = np.argmax(f1_scores)
        best_threshold = thresholds[best_idx] if best_idx < len(thresholds) else 0.5
        y_pred = (y_score > best_threshold).astype(int)

        results['accuracy'].append(float(accuracy_score(y_true, y_pred)))
        results['precision'].append(float(precision_score(y_true, y_pred, zero_division=0)))
        results['recall'].append(float(recall_score(y_true, y_pred, zero_division=0)))
        results['f1'].append(float(f1_score(y_true, y_pred, zero_division=0)))
        results['optimal_threshold'].append(float(best_threshold))

        try:
            results['roc_auc'].append(float(roc_auc_score(y_true, y_score)))
            pr, rc, _ = precision_recall_curve(y_true, y_score)
            results['pr_auc'].append(float(auc(rc, pr)))
        except Exception:
            results['roc_auc'].append(0.0)
            results['pr_auc'].append(0.0)

        fpr, tpr, _ = roc_curve(y_true, y_score)
        for target_fpr, key_name in [(1e-2, 'tpr_at_fpr_1e2'), (1e-3, 'tpr_at_fpr_1e3'), (1e-4, 'tpr_at_fpr_1e4')]:
            idx_arr = np.where(fpr <= target_fpr)[0]
            results[key_name].append(float(tpr[idx_arr[-1]]) if len(idx_arr) > 0 else 0.0)

    results_path = f'results_multi_seed{"_" + suffix if suffix else ""}.json'
    with open(results_path, 'w') as f:
        json.dump(results, f, indent=2)
    logger.info(f"\nMulti-seed results saved to {results_path}")

    _run_statistical_analysis(results, suffix)
    return results


def _run_statistical_analysis(results, suffix=""):
    """Compute mean, std, and 95% CI for multi-seed results."""
    from scipy import stats

    def compute_stats(values):
        values = np.array(values)
        n = len(values)
        mean = np.mean(values)
        std = np.std(values, ddof=1)
        ci = stats.t.interval(0.95, df=n - 1, loc=mean, scale=stats.sem(values))
        return mean, std, ci

    metric_names = {
        'accuracy': 'Accuracy', 'precision': 'Precision', 'recall': 'Recall',
        'f1': 'F1-Score', 'roc_auc': 'ROC AUC', 'pr_auc': 'PR AUC',
        'tpr_at_fpr_1e2': 'TPR@FPR=1e-2', 'tpr_at_fpr_1e3': 'TPR@FPR=1e-3',
        'tpr_at_fpr_1e4': 'TPR@FPR=1e-4',
    }

    logger.info("\n" + "=" * 60)
    logger.info(f"STATISTICAL SUMMARY (5-seed evaluation{', ' + suffix if suffix else ''})")
    logger.info("=" * 60)

    aggregated = {}
    for metric, name in metric_names.items():
        if metric in results and len(results[metric]) > 0:
            mean, std, ci = compute_stats(results[metric])
            aggregated[metric] = {'mean': float(mean), 'std': float(std), 'ci_95': [float(ci[0]), float(ci[1])]}
            logger.info(f"\n{name}:")
            logger.info(f"  Mean +/- Std: {mean:.8f} +/- {std:.8f}")
            logger.info(f"  95%% CI: [{ci[0]:.8f}, {ci[1]:.8f}]")

    logger.info("\n" + "=" * 60)

    agg_path = f'results_aggregated{"_" + suffix if suffix else ""}.json'
    with open(agg_path, 'w') as f:
        json.dump(aggregated, f, indent=2)
    logger.info(f"\nAggregated results saved to {agg_path}")
