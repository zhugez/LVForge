from transformers import PreTrainedTokenizerFast


class CustomDistilBertTokenizer(PreTrainedTokenizerFast):
    """Custom tokenizer for PE malware feature sequences.

    Extends DistilBERT tokenizer with domain-specific token processing
    for PE file sections, numeric values, DLL/API markers, and function names.
    """

    def __init__(self, *args, **kwargs):
        config = kwargs.pop("config", None)
        super().__init__(*args, **kwargs)
        self.config = config if config is not None else {}

    @classmethod
    def from_pretrained(cls, pretrained_model_name_or_path, *args, **kwargs):
        tokenizer = super().from_pretrained(pretrained_model_name_or_path, *args, **kwargs)
        tokenizer.config = getattr(tokenizer, "config", {})
        return tokenizer

    def tokenize(self, text, **kwargs):
        parts = text.strip().split()
        processed_tokens = []
        current_section = None

        for part in parts:
            if part.startswith('section_'):
                if '_name' in part:
                    current_section = part
                    processed_tokens.append(f"<SECTION_HEADER>{current_section}")
                elif '_size' in part or '_characteristics' in part:
                    processed_tokens.append(f"<{current_section}_{part.split('_')[-1].upper()}>")
                continue

            if part.isdigit():
                processed_tokens.append(f"<NUM_{len(part)}DIGIT>")
                continue

            if part.lower() in {'dll', 'api', 'export', 'import'}:
                processed_tokens.append(f"<{part.upper()}_MARKER>")
                continue

            if part.startswith('_'):
                processed_tokens.append(f"<FUNCTION_{part[1:].upper()}>")
                continue

            processed_tokens.append(part)

        return processed_tokens

    def process_token(self, token):
        if token == ":":
            return []
        if ":" in token:
            key, value = token.split(":", 1)
            return [key.strip(), value.strip()]
        return [token]


SPECIAL_TOKENS = {'<PAD>': 0, '<UNK>': 1, '<BOS>': 2, '<EOS>': 3}
