from dataclasses import dataclass
import logging
from typing import Optional, Dict, List, Tuple
import numpy as np
from collections import Counter

from .tokenizer import CustomDistilBertTokenizer, SPECIAL_TOKENS

logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)


@dataclass
class Preprocessor:
    """Unified data preprocessor for PE malware detection.

    Outputs numpy arrays. Use `to_jax()` to convert to JAX arrays.
    """

    tokenizer: CustomDistilBertTokenizer
    data_list: Optional[List[str]] = None
    data: Optional[List[str]] = None
    labels: Optional[List[int]] = None

    def _validate_data_loaded(self):
        if self.data is None:
            if self.data_list is None:
                raise ValueError("Data not loaded and data_list is None")
            self.load_data()

    def load_data(self) -> List[str]:
        if self.data_list is None:
            raise ValueError("No data provided.")
        self.data = [text for text in self.data_list]
        logger.info(f"Loaded {len(self.data)} samples")
        return self.data

    def clean_data(self) -> List[str]:
        self._validate_data_loaded()
        self.data = [text.strip("[]'\"") for text in self.data]
        return self.data

    def tokenize_rows(self) -> List[List[str]]:
        self._validate_data_loaded()
        return [self.tokenizer.tokenize(text) for text in self.data]

    def prepare_all_data(self, percentile: float = 95) -> Tuple[List[List[str]], np.ndarray, Dict[str, int], np.ndarray, int]:
        """Prepare data and return numpy arrays.

        Returns:
            tokenized_texts, labels, vocab, tokenized_data (np.ndarray), padding_length
        """
        self._validate_data_loaded()
        self.clean_data()
        tokenized_texts = self.tokenize_rows()
        vocab = self.build_vocab(tokenized_texts)
        padding_length = self.calculate_padding_length(tokenized_texts, percentile)
        tokenized_data = self.tokenize_text(tokenized_texts, vocab, padding_length)
        return tokenized_texts, self.labels, vocab, np.array(tokenized_data, dtype=np.int32), padding_length

    def to_jax(self, tokenized_data: np.ndarray, labels: np.ndarray):
        """Convert numpy arrays to JAX arrays on device."""
        import jax
        import jax.numpy as jnp
        data_jax = jnp.array(tokenized_data, dtype=jnp.int32)
        labels_jax = jnp.array(labels, dtype=jnp.int32)
        data_jax = jax.device_put(data_jax)
        labels_jax = jax.device_put(labels_jax)
        return data_jax, labels_jax

    def build_vocab(self, corpus: List[List[str]], min_freq: int = 1) -> Dict[str, int]:
        word_counts = Counter(word for sentence in corpus for word in sentence)
        vocab = SPECIAL_TOKENS.copy()
        next_idx = len(vocab)
        for word, count in word_counts.items():
            if count >= min_freq and word not in vocab:
                vocab[word] = next_idx
                next_idx += 1
        logger.info(f"Vocabulary size: {len(vocab)}")
        return vocab

    def tokenize_text(self, corpus: List[List[str]], vocab: Dict[str, int], max_len: int) -> List[List[int]]:
        tokenized = []
        for tokens in corpus:
            processed = [vocab['<BOS>']] + [vocab.get(token, vocab['<UNK>']) for token in tokens] + [vocab['<EOS>']]
            if len(processed) > max_len:
                processed = processed[:max_len - 1] + [vocab['<EOS>']]
            else:
                processed += [vocab['<PAD>']] * (max_len - len(processed))
            tokenized.append(processed)

        assert all(len(seq) == max_len for seq in tokenized), "Sequence lengths inconsistent!"
        logger.info(f"First sequence length: {len(tokenized[0])}, Last sequence length: {len(tokenized[-1])}")
        return tokenized

    def calculate_padding_length(self, corpus: List[List[str]], percentile: float = 95) -> int:
        lengths = [len(sentence) for sentence in corpus]
        padding_length = int(np.percentile(lengths, percentile, method="higher"))
        logger.info(f"Padding length ({percentile}th %%): {padding_length}")
        return padding_length
